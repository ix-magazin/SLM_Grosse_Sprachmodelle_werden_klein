# Small Language Models: Große Sprachmodelle werden klein
Code zum [Titelartikel](https://www.heise.de/select/ix/2025/4/2505111003573933334) von Christian Winkler, erschienen in [iX 04/2025](https://www.heise.de/select/ix/2025/4) und auf [heise online](https://www.heise.de/plus).

# iX-tract
- iX-tractFür große Sprachmodelle mit weniger als 10 Milliarden Parameter etabliert sich die Bezeichnung Small Language Model (SLM).
- Aufgrund der geringeren Größe SLMs lassen sich diese Modelle offline mit Grafikkarten, Laptop-CPUs oder auf Smartphones betreiben.
- Feintunen und RAG sind zwei beliebte Anwendungsfälle für SLMs, wo ihre Größe und die niedrigere Quantisierungsstufe eine höhere Performance erlaubt.
- Doch gerade bei diesen kleineren Modellen ist die Qualität der Trainingsdaten besonders wichtig. Im Vergleich mit den größeren Varianten neigen SLMs stärker zum Erfinden von Falschinformationen.
